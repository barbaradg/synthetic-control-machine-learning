# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TrpAzY8LMKT-I_UPWJAtmG-mA_Rl7c9R
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dateutil.relativedelta import relativedelta
from datetime import datetime, date
from tqdm import tqdm
import warnings
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

# Para los modelos
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import *
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.layers import Dropout
from keras.layers import Bidirectional
from keras.layers import Reshape
from keras.callbacks import EarlyStopping
from keras.preprocessing.sequence import TimeseriesGenerator

"""### Funciones"""

def WMAPE2 (true,predicted):
    WMAPE2=np.round((np.sum((np.abs(true-predicted)))/np.sum(true))*100,3)
    return (WMAPE2)

def df_to_X_y(df, window_size=3):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [a for a in df_as_np[i:i+window_size]]
    X.append(row)
    label = df_as_np[i+window_size][0]
    y.append(label)
  return np.array(X), np.array(y)

"""## Preparación data"""

df=pd.read_csv('DF_SYNTH2_km.csv')
df=df.iloc[:,1:92]
df=df[df['Semana']<=10] #hasta la semana 10, 6 a predecir
df=df.set_index('index')
df=df.drop(['plata'],axis=1)
test=df[df['Semana']>4]['suma']
#Escalamos variables
df_escaled=df
df_escaled=df_escaled.drop('rut_seller', axis=1)
df_escaled['suma']=df_escaled['suma'].replace(0,0.1)

df_escaled.iloc[:,3:13]=np.log(df_escaled.iloc[:,3:13]+1)
df_escaled['plata2']=np.log(df_escaled['plata2']+1)
df_escaled['suma']=np.log(df_escaled['suma']+1)

df_escaled=pd.concat([df_escaled.iloc[:,0:11],df_escaled.iloc[:,11:92]], axis=1)
train_sellers=pd.read_csv('train_sellers.csv')
test_sellers=pd.read_csv('test_sellers.csv')

#31 features
df_escaled=df_escaled[['suma','Semana','plata2','precio_desv','precio_median','precio_min','precio_75','SKU','tramo_6.0','tramo_9.0','G02','G06','G07','G09','G13','G16','G18','G21','G22','G0801','G0802','G1604','G1711','G1901','G2104','Otra cat','464100.0','471990.0','477399.0','702000.0','731001.0','cluster']]

#8 features
#df_escaled=df_escaled[['suma','Semana','plata2','precio_median','SKU','G18','G0802']]

train=df_escaled[df_escaled.index.isin(train_sellers.seller)]
test=df_escaled[df_escaled.index.isin(test_sellers.seller)]

X=train[train['Semana']<=4]
y=train[train['Semana']>4]
X_,y_=df_to_X_y(X,3)
x=[]
Y=[]
for i in range(len(X_)):
  if i%4==0:
    x.append(X_[i])
    Y.append(y_[i])
x=np.array(x)
Y=np.array(Y)

x.shape

early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min', min_delta=0.0001)
model = Sequential()
model.add(InputLayer((3, 32)))
model.add(LSTM(50, activation='relu', input_shape=(3, 32)))
model.add(Dropout(0.3))
model.add(Dense(1, 'linear'))
model.add(Dense(1, 'relu'))
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), loss='mae')
history=model.fit(x,Y,epochs=100,batch_size=16,validation_split=0.2,callbacks=[early_stop])
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

#SET TRAIN
test_train=train[train['Semana']>4]
X=train[train['Semana']<=4]
y=train[train['Semana']>4]
X_,y_=df_to_X_y(X,3)
x=[]
Y=[]
for i in range(len(X_)):
  if i%4==0:
    x.append(X_[i])
    Y.append(y_[i])
x=np.array(x)
Y=np.array(Y)
test_train['predicciones']=0
for i in np.unique(X.index):
    print(i)
    train_arr=np.array(X.loc[i])
    test_predictions = []
    first_eval_batch = train_arr[-3:]
    current_batch = first_eval_batch.reshape((1, 3, 32))
    hola=np.array(X.loc[i].drop('suma', axis=1))
    for j in range(5,11):
        current_pred = model.predict(current_batch)
        test_predictions.append(current_pred)
        current_pred=np.append(current_pred, hola[0])
        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)
    test_predictions=np.concatenate(test_predictions).ravel().tolist()
    test_train.loc[i,'predicciones']=np.exp(test_predictions)-1

#SET TEST
test_test=test[test['Semana']>4]
X=test[test['Semana']<=4]
y=test[test['Semana']>4]
X_,y_=df_to_X_y(X,3)
x=[]
Y=[]
for i in range(len(X_)):
  if i%4==0:
    x.append(X_[i])
    Y.append(y_[i])
x=np.array(x)
Y=np.array(Y)
test_test['predicciones']=0
for i in np.unique(X.index):
    print(i)
    train_arr=np.array(X.loc[i])
    test_predictions = []
    first_eval_batch = train_arr[-3:]
    current_batch = first_eval_batch.reshape((1, 3, 32))
    hola=np.array(X.loc[i].drop('suma', axis=1))
    for j in range(5,11):
        current_pred = model.predict(current_batch)
        test_predictions.append(current_pred)
        current_pred=np.append(current_pred, hola[0])
        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)
    test_predictions=np.concatenate(test_predictions).ravel().tolist()
    test_test.loc[i,'predicciones']=np.exp(test_predictions)-1

pred_train=test_train[['suma','predicciones','Semana']].reset_index()
pred_test=test_test[['suma','predicciones','Semana']].reset_index()

pred_train['suma']=np.exp(pred_train['suma'])-1
pred_test['suma']=np.exp(pred_test['suma'])-1

"""## Factor de corrección"""

#train
pred=np.array(pred_train['predicciones'])
y=np.array(pred_train['suma'])
resid=y-pred
n=len(y)
sigma=np.std(resid)
sm_lg=np.exp((1/n)*sum(resid**2)/(2*sigma**2))-((n-1)/(2*n))
pred_train['predicciones']= pred*sm_lg
#test
pred=np.array(pred_test['predicciones'])
y=np.array(pred_test['suma'])
resid=y-pred
n=len(y)
sigma=np.std(resid)
sm_lg=np.exp((1/n)*sum(resid**2)/(2*sigma**2))-((n-1)/(2*n))
pred_test['predicciones']= pred*sm_lg

pred_train=pred_train.set_index('index')
pred_test=pred_test.set_index('index')

MAPE_ENT=[]
WMAPE_ENT=[]
RMSE_ENT=[]

df_prueba=pred_train
for i in df_prueba.index:
    pred=np.array([df_prueba.loc[i,'predicciones']])
    real_ent=np.array([df_prueba.loc[i,'suma']])
    MAPE_ENT.append(np.round(mean_absolute_percentage_error(real_ent, pred)*100,3))
    WMAPE_ENT.append(WMAPE2(real_ent,pred))
    RMSE_ENT.append(np.sqrt(mean_squared_error(real_ent, pred)))
MAPE_TEST=[]
WMAPE_TEST=[]
RMSE_TEST=[]
df_prueba=pred_test
for i in pd.unique(df_prueba.index):
    pred=np.array(df_prueba.loc[i,'predicciones'])
    real_ent=np.array(df_prueba.loc[i,'suma'])
    MAPE_TEST.append(np.round(mean_absolute_percentage_error(real_ent, pred)*100,3))
    WMAPE_TEST.append(WMAPE2(real_ent,pred))
    RMSE_TEST.append(np.sqrt(mean_squared_error(real_ent, pred)))

BOXPLOTS7E=pd.DataFrame()
BOXPLOTS7E['MAPE_E']=MAPE_ENT
BOXPLOTS7E['WMAPE_E']=WMAPE_ENT
BOXPLOTS7E['RMSE_E']=RMSE_ENT
BOXPLOTS7E['MODELO']='LSTM'
BOXPLOTS7T=pd.DataFrame()
BOXPLOTS7T['MAPE_T']=MAPE_TEST
BOXPLOTS7T['WMAPE_T']=WMAPE_TEST
BOXPLOTS7T['RMSE_T']=RMSE_TEST
BOXPLOTS7T['MODELO']='LSTM'

np.round(BOXPLOTS7E.groupby('MODELO').agg(MAPE_E=('MAPE_E','mean'), WMAPE_E=('WMAPE_E','mean'),RMSE_E=('RMSE_E','mean')),2)

np.round(BOXPLOTS7T.groupby('MODELO').agg(MAPE_T=('MAPE_T','mean'), WMAPE_T=('WMAPE_T','mean'),RMSE_T=('RMSE_T','mean')),2)

BOXPLOTS7T.to_csv('lstm.csv')